{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "488f3471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ai_projects\\vision_language_model\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8ae7210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [02:18<00:00, 46.05s/it]\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "    \"paligemma-3b-pt-224\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    revision=\"bfloat16\",\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a93d1ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"paligemma-3b-pt-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb99e458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "prompt = \"caption es\"\n",
    "image = \"images/car.jpg\"\n",
    "img = Image.open(image).convert(\"RGB\")\n",
    "model_inputs = processor(text=prompt, images=img, return_tensors=\"pt\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46b595de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values tensor([[[[-0.4039, -0.3098, -0.2706,  ..., -0.8431, -0.8902, -0.9294],\n",
      "          [-0.3961, -0.3412, -0.4353,  ..., -0.8980, -0.9216, -0.9451],\n",
      "          [-0.5059, -0.4275, -0.3961,  ..., -0.9294, -0.9451, -0.9451],\n",
      "          ...,\n",
      "          [ 0.2000,  0.2000,  0.2078,  ...,  0.1922,  0.1608,  0.1451],\n",
      "          [ 0.2157,  0.2157,  0.2235,  ...,  0.2157,  0.1922,  0.1765],\n",
      "          [ 0.2314,  0.2314,  0.2235,  ...,  0.1373,  0.1373,  0.1451]],\n",
      "\n",
      "         [[-0.5608, -0.4667, -0.4039,  ..., -0.7569, -0.8039, -0.8431],\n",
      "          [-0.5608, -0.4980, -0.5765,  ..., -0.8118, -0.8353, -0.8588],\n",
      "          [-0.6706, -0.5843, -0.5451,  ..., -0.8431, -0.8588, -0.8588],\n",
      "          ...,\n",
      "          [ 0.1529,  0.1529,  0.1608,  ...,  0.1529,  0.1216,  0.1059],\n",
      "          [ 0.1686,  0.1686,  0.1765,  ...,  0.1765,  0.1529,  0.1373],\n",
      "          [ 0.1843,  0.1843,  0.1765,  ...,  0.0980,  0.0980,  0.1059]],\n",
      "\n",
      "         [[-0.5686, -0.4745, -0.4275,  ..., -0.7882, -0.8353, -0.8745],\n",
      "          [-0.5686, -0.5059, -0.5922,  ..., -0.8431, -0.8745, -0.8902],\n",
      "          [-0.6784, -0.5922, -0.5608,  ..., -0.8745, -0.8902, -0.8902],\n",
      "          ...,\n",
      "          [ 0.1529,  0.1529,  0.1608,  ...,  0.1294,  0.0980,  0.0824],\n",
      "          [ 0.1686,  0.1686,  0.1765,  ...,  0.1529,  0.1294,  0.1137],\n",
      "          [ 0.1843,  0.1843,  0.1765,  ...,  0.0745,  0.0745,  0.0824]]]])\n",
      "vision_tower_output tensor([[[-1.2188,  0.6719,  0.6172,  ..., -0.6484, -2.7812,  0.6797],\n",
      "         [-0.0101, -1.0859, -0.4160,  ...,  0.1465,  0.9141, -0.1729],\n",
      "         [-1.2500,  0.7812, -0.2715,  ..., -0.1621,  1.3281, -0.8047],\n",
      "         ...,\n",
      "         [ 0.2012, -0.4102,  0.2207,  ...,  0.3242, -0.0452, -0.1641],\n",
      "         [-0.7891,  0.6953,  0.4316,  ..., -0.1992, -0.3262,  0.8438],\n",
      "         [-0.2041,  1.1484, -0.4805,  ..., -0.3926, -0.9766,  0.5664]]],\n",
      "       dtype=torch.bfloat16)\n",
      "mmp tensor([[[ 1.5859, -0.3008,  0.1396,  ..., -0.3809,  0.2559,  0.2988],\n",
      "         [-0.9375,  0.2754,  0.4043,  ..., -0.2363,  0.3535,  0.0184],\n",
      "         [ 1.3594, -0.6133,  0.3047,  ..., -0.2334, -0.2578, -0.0591],\n",
      "         ...,\n",
      "         [-1.1797,  0.1621,  0.0476,  ..., -0.1670,  0.0859, -0.2441],\n",
      "         [-0.7930, -0.1826, -0.1855,  ...,  0.0025,  0.1367,  0.1875],\n",
      "         [-0.8906, -0.4492,  0.2480,  ...,  0.0618, -0.0396,  0.0723]]],\n",
      "       dtype=torch.bfloat16)\n",
      "logits tensor([[[ 6.2188, 12.6875,  4.3125,  ...,  7.9375,  8.0000,  7.8750]]],\n",
      "       dtype=torch.bfloat16)\n",
      "logits tensor([[[ 0.4629, 13.0000, -2.0312,  ...,  2.1875,  2.0156,  2.2344]]],\n",
      "       dtype=torch.bfloat16)\n",
      "logits tensor([[[ 1.2344, 13.7500,  9.6250,  ...,  3.0312,  2.9531,  3.0625]]],\n",
      "       dtype=torch.bfloat16)\n",
      "logits tensor([[[ 1.0156, 18.2500, -4.5625,  ...,  3.0625,  2.9062,  3.1094]]],\n",
      "       dtype=torch.bfloat16)\n",
      "logits tensor([[[-0.4688, 14.8125, -8.2500,  ...,  2.1562,  2.0312,  2.2344]]],\n",
      "       dtype=torch.bfloat16)\n",
      "logits tensor([[[-1.6094, 19.8750, -1.8516,  ...,  4.6562,  4.5312,  4.7188]]],\n",
      "       dtype=torch.bfloat16)\n",
      "logits tensor([[[-0.9375, 25.3750, -0.5234,  ...,  2.6250,  2.5156,  2.6719]]],\n",
      "       dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "generation = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n",
    "generation = generation[0][input_len:]\n",
    "decoded = processor.decode(generation, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51a2ff3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'persona estacionada en una calle'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
