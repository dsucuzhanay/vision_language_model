## Vision-Language Model

This project implements the **PaliGemma** vision-language model architecture from scratch. It includes:

- A Vision Transformer (ViT) based on **SigLIP** for visual encoding  
- The **Gemma** language model, including:
  - Grouped Query Attention (GQA)
  - Keyâ€“value (KV) caching for efficient autoregressive inference

Pretrained weights downloaded from the Hugging Face repository are used for inference.

![PaliGemma Architecture](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image4_W4FHDmx.original.png)

---

## Inference

Inference is demonstrated in `inference.ipynb`.

### Parameters

- **MODEL_PATH**  
  Path to the pretrained weights, tokenizer, and configuration files downloaded from Hugging Face.

- **IMAGE_PATH**  
  Path to the input image.

- **PROMPT**  
  Text prompt provided to the model.

- **MAX_TOKENS**  
  Maximum number of tokens generated by the model.

- **DO_SAMPLE**  
  - `True`: Perform top-p sampling  
  - `False`: Perform greedy decoding

- **TEMPERATURE**  
  Temperature parameter for top-p sampling.

- **TOP_P**  
  Top-p (nucleus) sampling parameter.

---

## Notes

- Use `test.ipynb` to download the PaliGemma repository assets from Hugging Face, including pretrained weights, tokenizer, and configuration files.
